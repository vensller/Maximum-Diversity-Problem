\documentclass[12pt]{article}

\usepackage{sbc-template}

\usepackage{graphicx,url}

\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{lscape}
     
\sloppy

\title{Desenvolvimento de uma solução para o problema Maximum Diversity Problem}

\author{Douglas de Souza Martins\inst{1}, Ivens Diego Müller\inst{1}, Tiago Funk\inst{1}}


\address{
  Departamento de Engenharia de Software\\
  Universidade do Estado de Santa Catarina -- Ibirama, SC -- Brazil
  \email{ivens.muller@edu.udesc.br, douglas.martins@edu.udesc.br, tiagoff.tf@gmail.com}
}

\begin{document} 

\maketitle

\begin{abstract}
  In this document, the problem Maximum Diversity Problem is presented where it consists of the representation of the following situation: A choice of N elements of a set of M elements where the sum of the distances between the elements is maximized. As the reading progresses, a more detailed explanation about the problem, its most frequently used solutions and the solution developed will be the main topic addressed in this text.
\end{abstract}
     
\begin{resumo} 
  Neste documento, será apresentado o problema Maximum Diversity Problem onde consiste na representação da seguinte situação: Uma escolha de N elementos de um conjunto de M elementos onde a soma das distâncias entre os elementos seja maximizada. Ao andamento da leitura, nota-se uma explicação mais detalhada sobre o problema, suas soluções mais utilizadas e a solução desenvolvida que será o principal tópico abordado neste texto.
\end{resumo}

\section{Introdução}
Este documento foi desenvolvido na disciplina de Projeto Integrador III, do curso de Bacharelado em Engenharia de Software, do Centro de Educação Superior do Alto Vale do Itajaí (CEAVI), da Universidade do Estado de Santa Catarina (UDESC).

Segundo \cite{silva2004experimental}, o Maximum Diversity Problem é um problema computacional que consiste, basicamente, em selecionar um subconjunto M de um conjunto N, cuja diversidade de M seja a maior possível.

Para resolução deste problema, são encontradas algumas meta-heurísticas, que são comumente utilizadas na resolução de problemas NP - Difíceis, pois entregam uma boa solução para um determinado problema, levando em consideração o tempo e o processamento.

Este estudo tem como objetivo analisar o MDP, implementar um algoritmo para resolução do problema e comparar seus resultados em relação ao estado da arte. Nas próximas seções serão vistas a análise do problema, uma descrição do estado da arte, a proposta de implementação deste trabalho, os resultados e a conclusão.

\section{Problema}

O problema, denominado Maximum Diversity Problem, é um desafio computacional que estuda a obtenção de M elementos dentre um conjunto de N elementos, onde a diversidade do subconjunto selecionado seja a maior possível. A diversidade, neste caso, é definida pelos atributos dos indivíduos onde o problema está sendo implantado. Considerando dois elementos pertencentes ao conjunto M, torna-se a distância entre eles calculada através da seguinte fórmula:

\begin{equation}
    d_{ij}=\sqrt{ \sum_{k=1}^{k} (s_{ik}-s_{jk})^{2} }
\end{equation}

Sendo i e j os dois elementos selecionados. Neste caso, a formulação dij representa a distância euclidiana entre estes elementos. Com as distâncias então estabelecidas, pegam-se os valores obtidos e transforma-se o MDP em um problema binário quadrático, onde a variável xi recebe como valor 1 se o elemento em questão foi selecionado ou 0 se caso não for. Então o problema fica definido matematicamente como:

\begin{equation}
    \text{Maximize }\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} d_{ij} x_{i} x_{j}
\end{equation}

\begin{equation}
    \text{Sujeito a } \sum_{i=1}^{n} x_{i} = m
\end{equation}
\begin{center}
    $x_i=\{0,1\}, 1 \leq i \leq n$
\end{center}

O Maximum Diversity Problem tem aplicações em problemas sociais, controle de poluição, engenharia genética, preservação ambiental, entre outros. Por exemplo, suponhamos que a empresa ABC necessita realizar a seleção de indivíduos para participarem de uma pesquisa sobre tecnologia. A empresa ABC tem a disposição para pesquisa 300 indivíduos, porém somente 30 devem ser selecionados, levando em consideração que o subconjunto selecionado deve ser o mais diversificado possível, levando em consideração a idade, profissão, sexo, entre outros aspectos. Para isso, pode ser aplicado um algoritmo de resolução de MDP, desde que a diversidade de cada indivíduo em relação a todos os outros esteja calculada.

\section{Estado-da-arte}

Existem muitos algoritmos computacionais para resolver este problema, mas há aqueles em que se destacam, onde aqui são denominados de métodos de estado-da-arte. Dentre os existentes, destacam-se:

\subsection{Métodos GRASP}
O método GRASP é multi-start, onde a cada iteração ele chama um método de construção e após isso, aplica um método de melhoria para encontrar um local ótimo, que seria a solução final para a iteração. \cite{silva2004experimental}, implementou o método GRASP utilizando os métodos de construção KLD, KLDv2, e MDI, e o BLS como método de melhoria. Abaixo, segue a descrição dos métodos mencionados anteriormente:

Métodos de construção:
\begin{itemize}
    \item KLD: O método de construção KLD trabalha com estimativas de contribuições entre um elemento e uma solução. Estas estimativas são construídas através da soma das maiores distâncias entre os elementos selecionados e não selecionados. Ao decorrer do algoritmo, forma-se uma lista com estes elementos. Após a lista ser concluída, é escolhida aleatoriamente 1 único elemento que será parte da solução desta construção. O algoritmo só termina quando todos os N elementos forem selecionados.
    \item KLDv2: O método de construção KLDv2 é apenas uma versão um pouco melhorada do método KLD, onde sua diferença é notada na lista de elementos de maiores distâncias, que, para formá-la, é utilizado um método adaptativo para isso.
    \item MDI: o método MDI também é um aperfeiçoamento de outro método, só que este vem do método KLDv2. O método KLDv2 calcula sua estimativa através das distâncias de elementos selecionados e não selecionados, já o método MDI obtêm sua estimativa através da soma das distâncias de um elemento com todos os outros elementos não selecionados e os elementos não selecionados com os elementos selecionados.
\end{itemize}

Método de melhoria:
\begin{itemize}
    \item BLS (Best improvement local search): Este método de melhoria seria, em curtas palavras, um método de busca local. Esta busca local é realizada por uma heurística que visa a melhor substituição de um elemento selecionado por um não selecionado. O método varre os elementos selecionados e verifica a melhor troca que trará a maior diversidade entre os elementos verificados, ou seja, maior aumento da função objetivo. O algoritmo só finaliza quando todas as iterações forem concluídas e não for detectado uma diversidade maior do que a já realizada.
\end{itemize}

\subsection{Métodos baseados em busca local}

\begin{itemize}
    \item ITS – Iterated Tabu Search: A Busca Tabu é uma meta-heurística que seu princípio básico é realizar uma busca local sempre que o algoritmo encontra um ótimo local. Este método permite realizar movimentos não aprimorados mas não permite voltar atrás para soluções já visitadas, onde este último é barrado pelo uso de memórias denominadas listas tabu, que realiza um histórico das buscas recentes.
    \item A\_VNS – Variable Neighborhood Search: O método VNS é, além de uma metaheurística, um método de otimização que realiza também uma busca local, muito similar ao método ITS descrito anteriormente. Este método explora uma vizinhança iterativa cada vez maior até encontrar um local ótimo para que um algoritmo de melhoria seja implementado ali. Ao localizar, todo o procedimento de expansão é repetido.
\end{itemize}

\subsection{Métodos baseados em população}

\begin{itemize}
    \item G\_SS: O método Scatter Search (SS) é um método baseado em população que é muito aplicado a problemas de otimização. Este método também se assemelha a Busca Tabu, já mencionada neste documento. Inclusive, este método pertence ao mesmo autor do algoritmo da Busca Tabu. De modo geral, o SS busca soluções o mais diversificadas possíveis com alta qualidade. De acordo com Jason Brownlee: “A estratégia envolve um processo iterativo, em que uma população de soluções candidatas diversas e de alta qualidade é particionada em subconjuntos e linearmente recombinada para criar centróides ponderados de vizinhanças baseadas em amostras. Os resultados da recombinação são refinados usando uma heurística embutida e avaliados no contexto do conjunto de referência quanto a serem ou não retidos.”
    \item MA: O algoritmo Memético tem como objetivo hibridar diferentes metaheuristicas para a resolução de um mesmo problema.  De modo geral, utilizam uma abordagem baseada em população, onde um conjunto de agentes cooperantes e concorrentes, utilizando busca local, são melhorados individualmente simultaneamente que interagem ocasionalmente.
\end{itemize}

\section{Desenvolvimento}

Para proposta do documento será desenvolvido um novo algoritmo que visa retornar a melhor solução do problema em um menor tempo. A forma como será abordado esse método consiste em estudar algoritmos já consagrados na literatura e mudar alguns componentes do algoritmo buscando formas de resolver o problema, mesclando técnicas ainda não utilizadas juntas e avaliar o seu desempenho, levando em conta buscar a melhor solução.

Para fazer isso será modificado o método já conhecido GRASP (nome autor) composto por métodos construtivos e seu método de melhoria da solução. Para tal modificação será subtistuido o seu principal método de melhoria, o BLS, para um tipo de busca local denominada Iterated Tabu Search, aonde espera-se obter a melhor solução utilizando uma forma diferente.

Para o desenvolvimento do trabalho, adotamos o método GRASP para implementação e obtenção das soluções das instâncias.
Conforme já falado anteriormente, o GRASP é executado e cria uma solução inicial a partir de um algoritmo construtivo e, depois desta solução inicial, ele aplica um método de melhoria até que não seja mais possível melhorar a solução.

Para isso, criamos uma classe chamada Grasp que, na sua criação, recebe por parâmetro a instância a ser analisada, uma estratégia de construção, uma estratégia de busca local e um número inteiro para ser definido como o máximo de iterações.

Portanto, tornou-se necessário a implementação de uma classe chamada Greedy e outra chamada LocalSearch. Cada uma destas classes receberá por parâmetro a sua estratégia para realização da solução. Estas estratégias serão explicadas nos tópicos posteriores.

Para a execução do GRASP, sua respectiva classe possui um método chamado execute, que ao final da execução retorna a melhor solução encontrada. O algoritmo implementado no método execute é simples, ele realiza um laço iniciando em 0 e indo até o número máximo de repetições que foi recebido como parâmetro na construção da classe. Nisso, em cada iteração, o GRASP cria uma nova solução através da classe Greedy e depois aplica uma busca local nesta solução criada, utilizando a classe LocalSearch. Após, é verificado se esta nova solução é a melhor encontrada até o momento e, caso for, é salva em uma variável para retornar no final da execução do método.

\subsection{Instâncias}

As instâncias são os possíveis casos do problema, por exemplo, se o nosso problema for somar dois números, cada par de números vai ser uma nova instância do problema para ser resolvida.

As instâncias que mais são utilizadas são as que estão disponiveis no site da optsicom (http://www.optsicom.es/mdp) e elas serão descritas rápidamente:

\begin{itemize}
    \item SOM: são 70 matrizes que estão preenchidas com números entre  e 9 gerados por um distribuição inteira uniforme.
    
    \item GKD: 145 matrizes preenchidas com valores que foram calculados com as distâncias euclidianas de pontos randomicamente gerados com suas coordenadas no intervalo de 0 a 10.
    
    \item MDG: Consiste em 100 matrizes com números selecionados randomicamente entre 0 e 10 em uma distribuição uniforme.
\end{itemize}


\subsection{Algoritmos gulosos de construção}

Abaixo, será descrito quais técnicas e métodos foram utlizados para construir a solução inicial do GRASP. Para o feito, foram implementados 7 algoritmos que montam uma solução inicial cada um da sua maneira.

\subsubsection{Algoritmo Aleatório}

O Algoritmo Aleatório, desenvolvido e utilizado em várias técnicas de construção de solução, apenas seleciona posições aleatórias para ser construída uma solução. É o algoritmo mais simples dentre os listados nesta sessão. Para cada posição escolhida, ele atribuirá o valor inteiro 1 no vetor de solução.

\subsubsection{Algoritmo Guloso}

O Algoritmo Guloso é um dos princípais métodos para para resolver problemas de otimização com o objetivo de encontrar um ótimo local. Ele se torna muito atrativo ao trabalhar com problemas NP-completo ou NP-difícil, pois ele obtêm uma solução aproximada da melhor em tempo polinomial. A cada iteração, o algoritmo guloso escolhe a solução que lhe parece mais "apetitosa" que lhe vem pela frente (neste caso, a solução apetitosa seria a que mais atende a F.O). Vale ressaltar, que o Algoritmo Guloso toma decisões com base nas informações que ele possui sem se importar com as consequências em que estas decisões podem acarretar. Ele também nunca volta atrás, suas decisões tomadas são definitivas em cada iteração. A grande vantagem de sua utilização é que ele é muito rápido e eficiente. No projeto do MDP, o Algoritmo Guloso foi apenas uma das estratégias utilizadas para construção de uma solução. O Algoritmo Guloso seleciona o próximo elemento onde ele possui o maior de diferença para selecionar. Diferença, neste contexto, refere-se a soma da diferença do elemento i com relação a todos os elementos que já foram selecionados.

\subsubsection{Guloso Ponderado}

O Algoritmo Guloso Ponderado funciona de forma semelhante ao próprio Algoritmo Guloso, porém sua diferença está no momento de obter o maior valor da soma das linhas. Para calcular esta soma, o Guloso Ponderado irá percorrer a matriz e na iteração i ele irá calcular um coeficiente, onde é uma soma ponderada aonde os elementos já selecionados possuem um peso x e os não selecionados possuem um peso y.  

\subsubsection{K Guloso}

O Algoritmo K Guloso recebe um valor "m" de elementos das quais ele precisa selecionar "k", onde o mesmo realiza a soma de cada linha da matriz calculada e coloca em uma lista, após finalizar, ele pega os "k" elementos maiores dessa lista e aleatóriamente escolhe um único elemento e atribui o valor booleano 'true' na lista de soluções.

\subsubsection{K Guloso Probabilístico}

O Algoritmo K Guloso Probabilístico realiza as primeiras operações de um algoritmo K Guloso comum, onde sua diferença está na hora de escolher, na lista de valores resultantes da soma das linhas, um elemento. No Algoritmo K Guloso comum, a escolha é feita aleatoriamente e grava no vetor de solução. Já neste caso, o K Guloso probabilístico soma todos os valores desta lista de somas e atribui-se 100(por cento) como todo o conjunto da lista. A probabilidade ocorre quando, cada valor desta lista é dividido pela soma total e atribuído a porcentagem deste valor em ser escolhido para a solução final. Nota-se que, quanto maior for a soma, maior será sua porcentagem de ser escolhido, e, consequentemente, quanto menor o valor da soma do conjuntos de elementos K que foram escolhidos, menor será a chance de se selecionado para a solução final.

\subsubsection{Alfa Guloso}

O Algoritmo Alfa Guloso é extremamente semelhante ao K Guloso, toda via ele não irá selecionar k elementos maiores da lista de somas e sim, uma porcentagem de elementos desta lista, ou seja, se o valor de alfa for 0,2, o algoritmo irá selecionar 20(por cento) de elementos desta lista de somas, e dentre este valor, ele irá escolher um elemento aleatório para adicionar ao vetor solução.

\subsubsection{Alfa Guloso Probabilístico}

Semelhante ao Algoritmo Alfa Guloso e K Guloso Probabilístico, o Algoritmo Alfa Guloso Probabilístico irá selecionar os elementos da lista de somas da mesma forma que o Alfa Guloso faz, toda via, desta lista, ele irá utilizar a mesma técnica que o K Guloso utiliza: soma todos os valores desta lista de soma e atribui uma porcentagem para cada valor com base na soma total realizada anteriormente. Logo, é visível notar que quanto maior o valor desta lista obtida, maior será sua probabilidade de ser selecionado.

\subsection{Algoritmos de busca local}
Como adotamos o GRASP como algoritmo para resolução das instâncias, precisamos de algoritmos de busca local para executarmos a segunda parte do GRASP. Após a construção da solução por meio do algoritmo guloso, a classe GRASP solicita a classe LocalSearch para que aplique uma estratégia de busca local para melhorar a solução inicial criada.

Portanto, a classe LocalSearch inicia sua execução com uma solução inicial e, a partir desta solução, ela aplica uma estratégia de busca de uma nova solução. Durante a busca, é verificado o par de posições do set da solução que pode ser trocado, realizando um swap dessas posições para verificar se há um incremento no valor total da solução. Para realizar este swap, é necessário que uma posição do set que esteja com o valor 1 vire 0 e a outra posição do set escolhida esteja com valor 0, para que esta possa ser definida com o valor 1. Foram implementadas duas estratégias de busca para incrementar a solução. Sendo elas a busca da primeira melhora e a busca da melhor melhora. A implementação da busca local pode ser verificada no pseudo-código \ref{localserachcode}.

\begin{algorithm}
    \caption{Local Search}\label{euclid}
    \begin{algorithmic}[1]
        \Procedure{localSerach(Solution s)}{}
            \While{True}
                \If {$\textit{strategy.movement}(s)$}
                    \State $remove \gets strategy.remove$.
                    \State $put \gets strategy.put$.
                    \State \textbf{s.swap(remove, put)}.
                    \Else \State \textbf{break};
                \EndIf
            \EndWhile
        \EndProcedure
    \end{algorithmic}
\label{localserachcode}
\end{algorithm}

Para as estratégias implementadas, criou-se uma interface chamada SearchStrategy. Esta interface possui um método movement, que recebe uma solução e retorna verdadeiro caso seja possível realizar um swap que incremente a função objetivo da solução recebida. Portanto, como foram implementadas duas estratégias de busca, criou-se as classes FirstImprovementSearch e BestImprovementSearch. Estas classes implementam a interface SearchStrategy e possuem comportamentos diferentes no método movement, que são explicados nas partes \ref{fis} e \ref{bis}.

Como tornou-se necessário verificar a possibilidade de swap em uma solução, implementou-se dentro da classe Solution uma função chamada Delta que, passando duas posições do set, a função calcula o valor Delta e retorna ao usuário este valor. O valor delta é a diferença do valor da função objetivo com o swap e o valor da função objetivo atual. Baseando-se nisso, criou-se outro método dentro da classe Solution chamado Swap, este método recebe um posição a ser setada para 0 e outra posição a ser setada para 1 e calcula o valor da função objetivo tendo como base este swap. 

\subsubsection{Busca da primeira melhora (FirstImprovementSearch)}\label{fis}
O algoritmo que realiza a busca da primeira melhora é executado até que ele encontre um par de posições do set para realizar o swap que o valor delta deste swap seja maior do que 0. Caso não tenha um par de posições possíveis para realizar o swap, a busca da primeira melhora retorna o valor false, fazendo com que termine a execução da busca local e passe para a próxima iteração do GRASP.

A execução do algoritmo é bem simples, são dois laços encadeados no partindo de 0 até o tamanho do set – 1. O primeiro laço verifica as posições do set que estão definidas com o valor 1 e, caso a posição atual esteja com este valor, ele passa para o laço 2. O laço 2 verifica as posições que estão com o valor 0 e, caso esteja, ele chama o método Delta da classe Solution para retornar o delta do swap destas posições. Caso o delta retornado for maior do que 0, o método movement retorna true.

\subsubsection{Busca da melhor melhora (BestImprovementSearch)}\label{bis}
O algoritmo que realiza a busca da melhor melhora é executado de maneira parecida ao algoritmo do tópico anterior. A diferença dele é que ele verifica todas as possibilidades de swap presentes na solução. 

Portanto, a execução dele se torna um pouco mais demorada em relação ao algoritmo anterior. Esta demora se da por que o algoritmo não para quando o delta retornado é maior do que 0, o algoritmo guarda o maior delta encontrado e as posições do swap para que, no final da execução desta estratégia, ela tenha guardado o melhor delta e o melhor par de posições para swap. Portanto, no fim de todas as iterações dos laços, a solução retorna true caso o maior delta encontrado for maior do que zero.

\section{Tuning}
Para obtermos flexibilidade na tentativa de encontrar a solução para cada instância, o sistema foi parametrizado por um todo. Existindo parâmetros para definição do algoritmo guloso que será escolhido, dos parâmetros específicos para cada algoritmo guloso, do total de repetições do GRASP, do algoritmo de busca local, entre outros.
Para isso, também se tornou necessário utilizar uma ferramenta chamada IRACE. O IRACE é uma ferramenta que automatiza a configuração dos parâmetros e define a melhor configuração possível baseado em seus cenários.

\subsection{Parâmetros}

Foram utilizados cinco parâmetros no sistema, que podem ser visto na tabela \ref{tabela_parametros}. Um que controla o número de repetições que algoritmo GRASP deve executar antes de finalizar e dar o resultado final. Um parâmetro que indica ao GRASP qual algoritmo construtivo deve ser utilizado para construir a primeira solução e depois tentar melhora-la. O parâmetro p possue duas funções, indica para o guloso ponderado que é o peso a diferença entre o elemento \textit{i} possue para os elementos selecionados e o peso para os não selecionados (1-p neste caso). Para os algoritmos alfa-guloso e alfa-guloso probabilistico, indica qual a proporção de alfa.

O parâmetro k indica para os algoritmos k-guloso e k-guloso probabilistico qual o valor da parcela que deve ser considerada para a escolha. O parâmetro search indica para o GRASP qual dos algoritmos de busca deve ser ulitizado.

\begin{table}[t]
	\centering
	\begin{tabular}{| c | p{3.5cm} | p{3.5cm} | p{3.5cm} | }
		\hline
		Parâmetro&Função&Possiveis valores&Dependencias\\ \hline
		nr&Indica o número de repetições que GRASP deve executar&Um inteiro entre 100 e 400&-\\ \hline
		const&Indica qual algoritmo construtivo deve ser executado&Aleatório (a), Guloso(g), Guloso ponderado (gp), k-guloso (kg), k-guloso probabilisitico (kgp), alfa-guloso (ag) e alfa-guloso probabilistico (agp)&-\\ \hline
		p&Indica uma probabilidade ou peso (Utilização diferente entre cada algoritmo)&Um real entre 0.1 e 1.0&const seja gp ou const seja ag ou const seja agp\\ \hline
		k&Indica parcela para algoritmo k-guloso&Um inteiro que pertence ao conjunto {1,5,10,15,20,25}&const seja kg ou const seja kgp\\ \hline
		search&Indica qual algoritmo de busca deve ser executado& Best Improvement Search (bis) e First Improvement Search (fis)&-\\ \hline
	\end{tabular}
	\caption{Parâmetros do sistema.}
	\label{tabela_parametros}
\end{table}


\subsection{Iterated Race (IRACE)}
Nesta subseção é apresentada os resultados obtidos para a calibração de parâmetros no IRACE. O experimento consistiui na execução de 150 Instancias \footnote{Só lembrando que esse texto não bate com os resultados finais, pois havia um bug que inválidou os resultados e precisa ser refeito.}, em quatro interações de 5000 experimentos cada.

A saída gerada pelo programa ao final da execução esta descrito na tabela \ref{tabela_resultado_irace}. A segunda opção foi escolhida pois esta representa uma média dos valores obtidos.

\begin{table}[t]
	\centering
	\begin{tabular}{| c | c | c | c | c |}
		\hline
		nr&const&search&p&k\\ \hline
		350&gp&fis&0.14&-\\ \hline
		307&gp&fis&0.13&-\\ \hline
		322&gp&fis&0.14&-\\ \hline
		236&gp&fis&0.12&-\\ \hline
	\end{tabular}
	\caption{Resultados gerados pelo IRACE.}
	\label{tabela_resultado_irace}
\end{table}

\section{Resultados}

Nesta seção é apresentado os resultados finais obtidos com execução do algoritmo com os parâmetros que o IRACE indicou serem os melhores. O experimento final consiste na execução de três vezes as seguintes instâncias. Vinte instâncias do tipo GKD-c, vinte do tipo MDG-a, vinte do tipo MGD-b e vinte do tipo SOM-b. Foram coletados dados sobre o tempo total de execução e solução final do problema, além de dados sobre cada uma das melhoras apresentadas pelo algoritmo e quanto tempo levou para essa melhora. Todos os experimento foram realizados em um computador com Intel Celeron(R) CPU B820 de 1.70GHz x 2 com 3,7 GiB de memória RAM e instalado o sistema operacional Ubuntu 16.04 LTS.

Para a apresentação dos dados obtidos para cada instância, sera montada uma tabela que resume as três execuções de cada instância, com as seguintes colunas. O nome da instância em Instancia, a melhor solução gerada por algum algoritmo na coluna em Estado da Arte, melhor solução gerada pelo algoritmo deste artigo em Melhor solução, diferença entre o estado da arte e a melhor solução em Desvio Melhor. A coluna Tempo Melhor indica quanto tempo levou em milisegundos a execução que deu a melhor solução, a Solução Média indica a média da solução gerada pelas três execuções do algoritmo na instância. Desvio Medio mostra a diferença entre o estado da arte e a solução média e por ultimo a média do tempo em milisegundos que as intâncias executam.

Na tabela \ref{tabela_gkd_c} esta listado os resultados obtidos para as instância GKD-c, o Desvio para o melhor e o Desvio para o resultado médio tendem a zero, ou seja, o algoritmo conseguiu chegar ao estado da arte ou muito próximo dele. O tempo ficou no intervalo entre 60 e 160 segundos.

Os resultados obtidos para as instâncias MDG-a esta listado na tabela \ref{tabela_mdg_a}, o desvio para o melhor foi maior, assim como o desvio para a média. Ficando no intervalo entre 0 e 57 para o desvio melhor e no tervalo de 2 e 60 para o desvio para a solução média. O tempo de execução ficou no intervalo entre 200 e 260 segundos.

Na tabela \ref{tabela_mdg_b} estão os resultados obtidos para as instâncias MDG-b, estas foram as que apresentaram a maior diferença entre a solução do algoritmo e estado da arte. O desvio para o melhor ficou no intervalo entre 0,1 e 6650 e o intervalo para o desvio para a solução média ficou no intervalo entre 0,1 e 7000. O tempo médio para estas instâncias ficou no intervalo de 210 segundos e 270 segundos.

Os valores obtidos para as instâncias SOM-b estão na tabela \ref{tabela_som_b}, o desvio para o melhor ficou no intervalo entre 0 e 61 e o desvio para a solução média ficou entre 0 e 75. O tempo médio ficou no intervalo de 1,5 e 2200 segundos.

\input{tabelas/tabela_gkd_c.tex}
\input{tabelas/tabela_mdg_a.tex}
\input{tabelas/tabela_mdg_b.tex}
\input{tabelas/tabela_som_b.tex}

\bibliographystyle{sbc}
\bibliography{sbc-template}

\end{document}
